---
description: "Enterprise quality gates, testing standards, and continuous validation requirements"
globs:
  - "**/*test*"
  - "**/*spec*"
  - "**/tests/**/*"
  - "**/test/**/*"
  - "**/spec/**/*"
  - "**/*.test.*"
  - "**/*.spec.*"
  - "**/cypress/**/*"
  - "**/e2e/**/*"
  - "**/integration/**/*"
  - "**/*quality*"
  - "**/quality/**/*"
---

# Enterprise Quality Gates & Testing Standards

## ðŸŽ¯ Quality-First Development (Never Skip)

**Before any code reaches production:**

### 1. Business Value Validation
- **User story testing**: Does this solve the actual user problem?
- **Acceptance criteria**: All business requirements verified
- **Success metrics**: Measurable outcomes defined and tracked
- **Edge case validation**: Boundary conditions and error scenarios tested

### 2. Technical Quality Gates
- **Security vulnerabilities**: Zero critical or high-severity issues
- **Performance benchmarks**: Response times within SLA requirements
- **Accessibility compliance**: WCAG 2.1 AA standards verified
- **Code coverage**: Minimum 80% for critical paths, 60% overall

## ðŸ”¬ Test Pyramid Strategy

### Unit Tests (70% of test effort)
```javascript
// Good: Focused, fast, isolated unit test
describe('calculateTax', () => {
  test('calculates correct tax for standard rate', () => {
    expect(calculateTax(100, 0.08)).toBe(8);
  });
  
  test('handles zero amount gracefully', () => {
    expect(calculateTax(0, 0.08)).toBe(0);
  });
  
  test('throws error for negative amounts', () => {
    expect(() => calculateTax(-100, 0.08)).toThrow();
  });
});
```

**Unit Test Requirements:**
- **Fast execution**: < 100ms per test
- **Isolated**: No external dependencies (databases, APIs, files)
- **Deterministic**: Same input always produces same output
- **Clear naming**: Test name explains scenario and expected outcome

### Integration Tests (20% of test effort)
```javascript
// Good: Tests component integration
describe('UserRegistration API', () => {
  test('creates user and sends welcome email', async () => {
    const userData = { email: 'test@example.com', name: 'Test User' };
    const response = await request(app)
      .post('/api/users')
      .send(userData);
    
    expect(response.status).toBe(201);
    expect(mockEmailService.sendWelcome).toHaveBeenCalledWith(userData.email);
  });
});
```

**Integration Test Requirements:**
- **Component interaction**: Test interfaces between modules
- **Database integration**: Real database with test data
- **API contracts**: Verify request/response formats
- **Third-party mocking**: Mock external services, test our integration

### End-to-End Tests (10% of test effort)
```javascript
// Good: Critical user journey test
describe('Complete Purchase Flow', () => {
  test('user can purchase product successfully', async () => {
    await page.goto('/products');
    await page.click('[data-testid="product-1"]');
    await page.click('[data-testid="add-to-cart"]');
    await page.click('[data-testid="checkout"]');
    // ... complete purchase flow
    await expect(page.locator('[data-testid="order-confirmation"]')).toBeVisible();
  });
});
```

**E2E Test Requirements:**
- **Critical user journeys**: Only test happy paths and major error scenarios
- **Real environment**: Production-like environment with actual services
- **Stable selectors**: Use data-testid, not fragile CSS selectors
- **Performance aware**: Monitor test execution time and flakiness

## ðŸš€ Performance Testing Standards

### Load Testing Requirements
- **Baseline performance**: Document current performance characteristics
- **Load targets**: Test at 2x expected peak traffic
- **Stress testing**: Find breaking point and graceful degradation
- **Endurance testing**: Verify performance over extended periods

### Performance Metrics
- **Response time**: P50 < 200ms, P95 < 500ms, P99 < 1000ms
- **Throughput**: Requests per second targets by endpoint
- **Error rate**: < 0.1% error rate under normal load
- **Resource utilization**: CPU < 70%, Memory < 80%, Disk I/O monitored

## ðŸ”’ Security Testing Integration

### Static Analysis Security Testing (SAST)
- **Code scanning**: Automated vulnerability detection in CI/CD
- **Dependency scanning**: Known vulnerability databases checked
- **Secret detection**: No hardcoded secrets or credentials
- **Code quality**: SonarQube or similar for maintainability metrics

### Dynamic Analysis Security Testing (DAST)
- **Penetration testing**: Automated security scanning
- **Authentication testing**: Verify proper access controls
- **Input validation**: SQL injection, XSS, command injection testing
- **Session management**: Token handling and session security

## â™¿ Accessibility Testing Requirements

### Automated Testing
```javascript
// Good: Accessibility testing in unit tests
import { axe, toHaveNoViolations } from 'jest-axe';

test('login form is accessible', async () => {
  const { container } = render(<LoginForm />);
  const results = await axe(container);
  expect(results).toHaveNoViolations();
});
```

### Manual Testing Checklist
- [ ] **Keyboard navigation**: Complete all tasks using only keyboard
- [ ] **Screen reader**: Test with NVDA, JAWS, or VoiceOver
- [ ] **High contrast**: Test in Windows High Contrast Mode
- [ ] **Zoom testing**: Usable at 200% zoom level
- [ ] **Color blindness**: Test with color blindness simulators

## ðŸ”„ Continuous Quality Monitoring

### CI/CD Quality Gates
```yaml
# Example: Quality gates in CI/CD pipeline
quality_gates:
  - name: "Unit Tests"
    threshold: "95% pass rate"
    blocking: true
    
  - name: "Code Coverage" 
    threshold: "80% coverage"
    blocking: true
    
  - name: "Security Scan"
    threshold: "0 high/critical vulnerabilities"
    blocking: true
    
  - name: "Performance Tests"
    threshold: "P95 < 500ms"
    blocking: false  # Warning only
```

### Quality Metrics Dashboard
- **Test results**: Pass/fail rates, trend analysis
- **Code coverage**: Line and branch coverage metrics
- **Performance trends**: Response time and throughput over time
- **Security posture**: Vulnerability counts and resolution time

## ðŸ§ª Test Data Management

### Test Data Strategy
- **Synthetic data**: Generated test data for consistent testing
- **Production-like**: Realistic data volumes and complexity
- **Data privacy**: No real customer data in test environments
- **Data refresh**: Regular updates to keep tests relevant

### Test Environment Management
- **Environment parity**: Test environments mirror production
- **Data isolation**: Each test runs with clean, known data state
- **Parallel execution**: Tests can run simultaneously without conflicts
- **Teardown**: Proper cleanup after test execution

## ðŸ” Code Review Quality Standards

### Review Checklist
- [ ] **Business logic**: Does code solve the intended problem?
- [ ] **Security**: No vulnerabilities or security anti-patterns
- [ ] **Performance**: No obvious performance issues
- [ ] **Maintainability**: Code is readable and well-structured
- [ ] **Testing**: Adequate test coverage for new functionality
- [ ] **Documentation**: Complex logic explained in comments or docs

### Review Process
1. **Automated checks**: All CI/CD checks must pass
2. **Peer review**: At least one experienced team member
3. **Domain expert**: Relevant specialist (security, performance, etc.)
4. **Approval criteria**: All reviewers approve, no outstanding issues

## ðŸ“Š Quality Metrics & KPIs

### Development Quality
- **Defect density**: Bugs per lines of code
- **Test coverage**: Percentage of code covered by tests
- **Code duplication**: Percentage of duplicated code blocks
- **Technical debt**: SonarQube technical debt ratio

### Production Quality
- **Availability**: System uptime percentage
- **Error rate**: Production error frequency
- **Performance**: Response time percentiles
- **User satisfaction**: Customer satisfaction scores

## ðŸš¨ Quality Gate Failures

### Critical Issues (Block Deployment)
- **Security vulnerabilities**: High/critical security issues
- **Test failures**: Critical user journey tests failing
- **Performance regression**: Significant performance degradation
- **Accessibility failures**: WCAG compliance violations

### Warning Issues (Monitor Closely)
- **Code coverage drops**: Below established thresholds
- **Performance degradation**: Minor but measurable performance impact
- **Technical debt increase**: Maintainability metrics declining
- **Dependency vulnerabilities**: Low/medium security issues

## ðŸ¤ Cross-Team Quality Collaboration

### Quality Review Process
**Code Reviewer â†’ Product Manager**: "Does this implementation meet business requirements?"
**Code Reviewer â†’ UX Designer**: "Does this match the intended user experience?"
**Code Reviewer â†’ Responsible AI**: "Any bias or accessibility concerns?"
**Code Reviewer â†’ DevOps**: "Any deployment or operational concerns?"

### Escalation Triggers
- **Quality vs. timeline tradeoffs**: Stakeholder decision required
- **Technical debt accumulation**: Architecture review needed
- **Performance requirements**: Infrastructure scaling decisions
- **Accessibility compliance**: Legal and compliance implications

## ðŸ“ˆ Continuous Improvement

### Quality Retrospectives
- **Monthly quality reviews**: Analyze trends and identify improvements
- **Post-incident analysis**: Learn from production issues
- **Test effectiveness**: Identify gaps in test coverage
- **Process optimization**: Streamline quality gates without compromising standards

### Quality Investment
- **Tool automation**: Invest in better testing and analysis tools
- **Team training**: Keep team skills current with quality practices
- **Quality coaching**: Pair programming and mentoring for quality
- **Knowledge sharing**: Document and share quality practices

**Remember**: Quality is everyone's responsibility, not just QA. Build quality into the development process from day one.